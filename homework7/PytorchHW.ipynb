{"nbformat":4,"nbformat_minor":4,"metadata":{"notebookPath":"PytorchHW.ipynb","language_info":{"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}},"notebookId":"5191024f-32e4-4097-9909-4c3bc97d8514","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"ydsNotebookPath":"PytorchHW.ipynb"},"cells":[{"cell_type":"code","source":"#!g1.1\nimport torch","metadata":{"cellId":"j8361qeypgdcoww1tj303","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#!g1.1\nimport pandas as pd\nimport numpy as np","metadata":{"cellId":"3tc0g4qxj5vzyl49yko3js","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#!g1.1\n\nfrom torch.utils.data import Dataset, DataLoader, random_split","metadata":{"cellId":"yaw4355auhg6ffok5vuiem","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#!g1.1\n\nfrom numpy import vstack\nfrom numpy import argmax\nfrom pandas import read_csv\nfrom sklearn.metrics import accuracy_score\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import Compose\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import Normalize\nfrom torch.utils.data import DataLoader\nfrom torch.nn import Conv2d\nfrom torch.nn import MaxPool2d\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Softmax\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\n\nEMNIST_NUM_CLASSES=26\n\nclass EMNISTCNN(Module):\n    def __init__(self, n_channels):\n        super(EMNISTCNN, self).__init__()\n        # input to first hidden layer\n        self.hidden1 = Conv2d(n_channels, 32, (3,3))\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # first pooling layer\n        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n        # second hidden layer\n        self.hidden2 = Conv2d(32, 32, (3,3))\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # second pooling layer\n        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n        # fully connected layer\n        self.hidden3 = Linear(5*5*32, 100)\n        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n        self.act3 = ReLU()\n        # output layer\n        self.hidden4 = Linear(100, EMNIST_NUM_CLASSES)\n        xavier_uniform_(self.hidden4.weight)\n        self.act4 = Softmax(dim=1)\n \n    def forward(self, X):\n        # input to first hidden layer\n        X = self.hidden1(X)\n        X = self.act1(X)\n        X = self.pool1(X)\n        # second hidden layer\n        X = self.hidden2(X)\n        X = self.act2(X)\n        X = self.pool2(X)\n        # flatten\n        X = X.view(-1, 4*4*50)\n        # third hidden layer\n        X = self.hidden3(X)\n        X = self.act3(X)\n        X = self.hidden4(X)\n        X = self.act4(X)\n        return X","metadata":{"cellId":"yv9nhixn77kvvmz25qifl","trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#!g1.1\nclass EMNISTCSVLoader(Dataset):\n    def __init__(self, path, batch_size=1, shuffle=False, num_workers=0):\n        super(EMNISTCSVLoader, self).__init__()\n        df = pd.read_csv(path, header=None)\n        self.X = df.values[:,1:].astype('float32') / 255\n        self.X = self.X.reshape(-1, 1, 28, 28)\n        self.Y = df.values[:,0]\n        self.Y = self.Y.reshape(-1) - 1\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx: int):\n        return [self.X[idx], self.Y[idx]]\n    \n    def get_splits(self, n_test=0.33):\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        return random_split(self, [train_size, test_size])","metadata":{"cellId":"inub3x122wcj09kae91rn","trusted":true},"outputs":[],"execution_count":71},{"cell_type":"code","source":"#!g1.1\ndef prepare_letters_dataset(path):\n    dataset = EMNISTCSVLoader(path, shuffle=True)\n    train, test = dataset.get_splits()\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=32)\n    return train_dl, test_dl","metadata":{"cellId":"ltta8cgjj6lwynpurudsgn","trusted":true},"outputs":[],"execution_count":72},{"cell_type":"code","source":"#!g1.1\nmodel = EMNISTCNN(1)","metadata":{"cellId":"njrsy2cye7qovvknz1nph","trusted":true},"outputs":[],"execution_count":73},{"cell_type":"code","source":"#!g1.1\npath = \"emnist-letters-train.csv\"\ntrain_dl, test_dl = prepare_letters_dataset(path)","metadata":{"cellId":"4t2h2g0rcd9qfy2at2jhi","trusted":true},"outputs":[],"execution_count":74},{"cell_type":"code","source":"#!g1.1\n\ndef train_model(train_dl, model):\n    criterion = CrossEntropyLoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    for epoch in range(50):\n        for i, (inputs, targets) in enumerate(train_dl):\n            optimizer.zero_grad()\n            yhat = model(inputs)\n            loss = criterion(yhat, targets)\n            loss.backward()\n            optimizer.step()\n\n\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        yhat = model(inputs)\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        yhat = argmax(yhat, axis=1)\n        actual = actual.reshape((len(actual), 1))\n        yhat = yhat.reshape((len(yhat), 1))\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    acc = accuracy_score(actuals, predictions)\n    return acc","metadata":{"cellId":"x4xa7sbfecj66p42eemrud","trusted":true},"outputs":[],"execution_count":77},{"cell_type":"code","source":"#!g1.1\ndef train_model(train_dl, model):\n    criterion = CrossEntropyLoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    for epoch in range(50):\n        for i, (inputs, targets) in enumerate(train_dl):\n            optimizer.zero_grad()\n            yhat = model(inputs)\n            loss = criterion(yhat, targets)\n            loss.backward()\n            optimizer.step()","metadata":{"cellId":"zfz2ulckbtbxp30zzc9rrr","trusted":true},"outputs":[],"execution_count":80},{"cell_type":"code","source":"#!g1.1\ntrain_model(train_dl, model)","metadata":{"cellId":"s5pv7i8v7gewcl5irbv40b","trusted":true},"outputs":[],"execution_count":81},{"cell_type":"code","source":"#!g1.1\nevaluate_model(train_dl, model)","metadata":{"cellId":"dkf6vl89w7auz25lfq9hh","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"0.7245529111200752"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"276905l9h2xcf602a32yd7"},"outputs":[],"execution_count":null}]}